{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALTjasonspeed\\.conda\\envs\\JJAZ\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_prompt = \"Please condense the following expert opinion text into a concise summary with these requirements:\\n \\\n",
    "1. Each sentence must contain no more than 20 words.\\n \\\n",
    "2. The entire summary must be under 80 words.\\n \\\n",
    "3. Retain the original key points and arguments without adding extra explanations, background, or references.\\n \\\n",
    "Text:\\n\"\n",
    "answer_prompt = \"Please provide your expert opinion on the following scenario, formatted as follows: \\\n",
    "1. Each sentence must contain no more than 20 words. \\\n",
    "2. The entire answer must be under 50 words. \\\n",
    "3. Express only your opinion without additional explanations, background, references, or disclaimers. \\\n",
    "4. Avoid using phrases like 'I think' or 'I believe'. \\\n",
    "Scenario:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPTResponse(text, is_condense=False):\n",
    "    client = OpenAI(\n",
    "    api_key=\"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\"\n",
    "    )\n",
    "    \n",
    "    if is_condense:\n",
    "        prompt = condense_prompt + text\n",
    "    else:\n",
    "        prompt = answer_prompt + text\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=\"\",\n",
    "        input=prompt,\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "def DPSKResponse(text, is_condense=False):\n",
    "    client = OpenAI(api_key=\"sk-65e478f6033944a28ba38274d6785f07\", base_url=\"https://api.deepseek.com\")\n",
    "    if is_condense:\n",
    "        prompt = condense_prompt + text\n",
    "    else:\n",
    "        prompt = answer_prompt + text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "    ],\n",
    "    stream=False\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(prompt w/o word count limitation) Ans:\n",
    "The lab should have a clear authorship and credit policy in place to prevent situations like this. In this case, the post-doc should not take credit for the work that was methodically completed by the graduate student. The credit should go to the graduate student for the key breakthrough, as they were the one who conducted the essential testing and validation. The decisional process should involve an open discussion between the graduate student, the post-doc, and the PI, where the contributions to the research are transparently reviewed. The PI should consider the lab notes, timelines, and the nature of the contributions made by each individual. This scenario highlights the importance of documenting contributions and maintaining a culture of collaboration and respect, ensuring that all parties recognize and acknowledge each other’s efforts appropriately.\n",
    "\n",
    "(new prompt w/ word count limitation) Ans:\n",
    "The post-doc should transparently acknowledge your contribution. Proper credit must reflect actual work done. The lab should establish clear ownership policies. Discussing concerns with the PI is essential. Open communication can prevent future conflicts. Fair recognition fosters a collaborative environment. Ensure all contributions are documented clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\ALTjasonspeed\\AppData\\Local\\Temp\\ipykernel_7540\\1407123058.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dilemma = df.iloc[i][0]\n",
      "C:\\Users\\ALTjasonspeed\\AppData\\Local\\Temp\\ipykernel_7540\\1407123058.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  expert = df.iloc[i][1]\n",
      "100%|██████████| 10/10 [00:40<00:00,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity score: 0.6831197142601013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT\n",
    "df = pd.read_csv('../docs/test_csv.csv')\n",
    "eva_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "score = 0\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    dilemma = df.iloc[i][0]\n",
    "    expert = df.iloc[i][1]\n",
    "    sentence1 = GPTResponse(expert, is_condense=True)  # Get the condensed expert opinion\n",
    "    sentence2 = GPTResponse(dilemma, is_condense=False)  # Get the gpt answer\n",
    "    \n",
    "    embedding1 = eva_model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = eva_model.encode(sentence2, convert_to_tensor=True)\n",
    "    cosine_sim = util.cos_sim(embedding1, embedding2)\n",
    "    score += cosine_sim\n",
    "\n",
    "score /= len(df)\n",
    "print(f\"Average similarity score: {score.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\ALTjasonspeed\\AppData\\Local\\Temp\\ipykernel_22580\\1929261553.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dilemma = df.iloc[i][0]\n",
      "C:\\Users\\ALTjasonspeed\\AppData\\Local\\Temp\\ipykernel_22580\\1929261553.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  expert = df.iloc[i][1]\n",
      "100%|██████████| 10/10 [03:05<00:00, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity score: 0.6466969847679138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DeepSeekV3\n",
    "df = pd.read_csv('../docs/test_csv.csv')\n",
    "eva_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "score = 0\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    dilemma = df.iloc[i][0]\n",
    "    expert = df.iloc[i][1]\n",
    "    sentence1 = DPSKResponse(expert, is_condense=True)  # Get the condensed expert opinion\n",
    "    sentence2 = DPSKResponse(dilemma, is_condense=False)  # Get the gpt answer\n",
    "    \n",
    "    embedding1 = eva_model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = eva_model.encode(sentence2, convert_to_tensor=True)\n",
    "    cosine_sim = util.cos_sim(embedding1, embedding2)\n",
    "    score += cosine_sim\n",
    "\n",
    "score /= len(df)\n",
    "print(f\"Average similarity score: {score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

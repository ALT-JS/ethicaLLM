{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "c:\\Users\\19664\\anaconda3\\envs\\ai_latest\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Topic                                Dilemma Name & Link  \\\n",
      "0  Allocating Credit                         Who Gets the Credit? (PDF)   \n",
      "1  Allocating Credit                        But That Was My Idea! (PDF)   \n",
      "2  Allocating Credit  Replacing a First Author on a Second Submissio...   \n",
      "3  Allocating Credit        The Tyrannical Principal Investigator (PDF)   \n",
      "4  Allocating Credit                      The Overly Nice Advisor (PDF)   \n",
      "\n",
      "                                Dilemma Description   \\\n",
      "0  I joined a lab during graduate school and was ...   \n",
      "1  Graduate students A and B are working on somew...   \n",
      "2  David is a new postdoc in Dr. Goliaths lab. Up...   \n",
      "3  A PI moves his lab to a different university, ...   \n",
      "4  George Washington is one of two postdocs worki...   \n",
      "\n",
      "                                   ChatGPT's Answer   \\\n",
      "0  This situation involves ethical concerns relat...   \n",
      "1  This situation raises significant ethical conc...   \n",
      "2  Davids handling of the manuscript raises ethic...   \n",
      "3  No, the PIs actions are unethical and violate ...   \n",
      "4  Dr. Bigs actions are ethically problematic and...   \n",
      "\n",
      "                                   Deepseek's Answer  \\\n",
      "0  This situation highlights a common but challen...   \n",
      "1  This scenario raises important questions about...   \n",
      "2  This scenario raises important questions about...   \n",
      "3  This scenario raises significant ethical conce...   \n",
      "4  This scenario highlights ethical concerns abou...   \n",
      "\n",
      "                                     Claude's Answer  \\\n",
      "0  This situation highlights the complex dynamics...   \n",
      "1  This scenario illustrates the delicate issues ...   \n",
      "2  This situation raises several important author...   \n",
      "3  This PI's actions raise serious ethical concer...   \n",
      "4  This situation raises several ethical concerns...   \n",
      "\n",
      "                         Expert Opinion (From Jesse)  \\\n",
      "0  We were surprised to discover that the literat...   \n",
      "1  Disputes over assigning credit in science are ...   \n",
      "2  A key source of ethical guidance in resolving ...   \n",
      "3  At first blush, this PI certainly seems to be ...   \n",
      "4  Dr. Big might be a fine scientist, but hes not...   \n",
      "\n",
      "                            GPT-Summary (From Jesse)  \\\n",
      "0  The literature on credit allocation rules in s...   \n",
      "1  \"Disputes over scientific credit, often center...   \n",
      "2  The ethical dilemma of authorship in scientifi...   \n",
      "3  The PI in question appears vindictive, rewriti...   \n",
      "4  \"Dr. Big may be a skilled scientist, but he fa...   \n",
      "\n",
      "                                       expert_scores  \\\n",
      "0  LABEL_0=0.9582;LABEL_1=0.0019;LABEL_2=0.9373;L...   \n",
      "1  LABEL_0=0.9779;LABEL_1=0.0049;LABEL_2=0.0050;L...   \n",
      "2  LABEL_0=0.0014;LABEL_1=0.0055;LABEL_2=0.0022;L...   \n",
      "3  LABEL_0=0.0041;LABEL_1=0.0029;LABEL_2=0.8781;L...   \n",
      "4  LABEL_0=0.0016;LABEL_1=0.0009;LABEL_2=0.4485;L...   \n",
      "\n",
      "                                       model1_scores  \\\n",
      "0  LABEL_0=0.9452;LABEL_1=0.0075;LABEL_2=0.9927;L...   \n",
      "1  LABEL_0=0.0010;LABEL_1=0.0064;LABEL_2=0.9847;L...   \n",
      "2  LABEL_0=0.9647;LABEL_1=0.0024;LABEL_2=0.9898;L...   \n",
      "3  LABEL_0=0.0022;LABEL_1=0.0024;LABEL_2=0.9905;L...   \n",
      "4  LABEL_0=0.0037;LABEL_1=0.9572;LABEL_2=0.9458;L...   \n",
      "\n",
      "                                       model2_scores  \\\n",
      "0  LABEL_0=0.9578;LABEL_1=0.0014;LABEL_2=0.9496;L...   \n",
      "1  LABEL_0=0.9035;LABEL_1=0.0026;LABEL_2=0.0101;L...   \n",
      "2  LABEL_0=0.0011;LABEL_1=0.0007;LABEL_2=0.9379;L...   \n",
      "3  LABEL_0=0.0033;LABEL_1=0.0028;LABEL_2=0.9841;L...   \n",
      "4  LABEL_0=0.9642;LABEL_1=0.0024;LABEL_2=0.9818;L...   \n",
      "\n",
      "                                       model3_scores  \n",
      "0  LABEL_0=0.0010;LABEL_1=0.0008;LABEL_2=0.9193;L...  \n",
      "1  LABEL_0=0.9803;LABEL_1=0.0028;LABEL_2=0.9799;L...  \n",
      "2  LABEL_0=0.0015;LABEL_1=0.0006;LABEL_2=0.0024;L...  \n",
      "3  LABEL_0=0.0033;LABEL_1=0.0103;LABEL_2=0.9923;L...  \n",
      "4  LABEL_0=0.9808;LABEL_1=0.0047;LABEL_2=0.9896;L...  \n"
     ]
    }
   ],
   "source": [
    "# Use a dataset to train the labels and compare.\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"LLMs Answers.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    data = f.read()\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"MMADS/MoralFoundationsClassifier\",\n",
    "    tokenizer=\"MMADS/MoralFoundationsClassifier\",\n",
    "    return_all_scores=True,\n",
    "    function_to_apply=\"sigmoid\",\n",
    "    truncation=True,       \n",
    "    max_length=512         \n",
    ")\n",
    "\n",
    "\n",
    "def classify_text_all_scores(text):\n",
    "\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    results = classifier(text)  \n",
    "    label_scores = results[0]   \n",
    "    \n",
    "    scores_str = \";\".join([f\"{item['label']}={item['score']:.4f}\" for item in label_scores])\n",
    "    return scores_str\n",
    "\n",
    "df[\"expert_scores\"] = df[\"Expert Opinion (From Jesse)\"].apply(classify_text_all_scores)\n",
    "df[\"model1_scores\"] = df[\"ChatGPT's Answer \"].apply(classify_text_all_scores)\n",
    "df[\"model2_scores\"] = df[\"Deepseek's Answer\"].apply(classify_text_all_scores)\n",
    "df[\"model3_scores\"] = df[\"Claude's Answer\"].apply(classify_text_all_scores)\n",
    "\n",
    "output_path = \"LLMs Answers_with_all_scores.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.556069016456604, 0.366840660572052, 0.7436351776123047, 0.7610009908676147, 0.6180353164672852, 0.7156317234039307, 0.4273454248905182, 0.3276230990886688, 0.6225638389587402, 0.6078225374221802, 0.7586894035339355, 0.6929235458374023, 0.5986394882202148, 0.6055260300636292, 0.7045950293540955] [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1] [0.44605299830436707, 0.3555789589881897, 0.6455888748168945, 0.744238018989563, 0.5798215866088867, 0.7657429575920105, 0.5138099789619446, 0.5363022089004517, 0.5810044407844543, 0.6067180037498474, 0.5938402414321899, 0.5310254096984863, 0.6078423261642456, 0.6688383221626282, 0.7309655547142029] [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] [0.5126893520355225, 0.47484835982322693, 0.6913854479789734, 0.7563563585281372, 0.5765470266342163, 0.7236986756324768, 0.4507281184196472, 0.47473403811454773, 0.6010711789131165, 0.5767053365707397, 0.7423221468925476, 0.603062629699707, 0.5895752906799316, 0.5499739646911621, 0.6792951822280884] [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Directly compare to judge whether it is a good response and evaluate through the propotion of good responses.\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from io import StringIO\n",
    "\n",
    "with open(\"LLMs Answers.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    data = f.read()\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "######\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "######\n",
    "threshold = 0.7\n",
    "\n",
    "model1_scores, model2_scores, model3_scores = [], [], []\n",
    "model1_statuses, model2_statuses, model3_statuses = [], [], []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    expert_text = row[\"Expert Opinion (From Jesse)\"]\n",
    "    m1 = row[\"ChatGPT's Answer \"]\n",
    "    m2 = row[\"Deepseek's Answer\"]\n",
    "    m3 = row[\"Claude's Answer\"]\n",
    "    \n",
    "    expert_embedding = model.encode(expert_text, convert_to_tensor=True)\n",
    "    \n",
    "    scores = []\n",
    "    statuses = []\n",
    "    \n",
    "    for model_answer in [m1, m2, m3]:\n",
    "        answer_embedding = model.encode(model_answer, convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(expert_embedding, answer_embedding).item()\n",
    "        scores.append(similarity)\n",
    "        statuses.append(1 if similarity >= threshold else 0)\n",
    "    \n",
    "    model1_scores.append(scores[0])\n",
    "    model2_scores.append(scores[1])\n",
    "    model3_scores.append(scores[2])\n",
    "    model1_statuses.append(statuses[0])\n",
    "    model2_statuses.append(statuses[1])\n",
    "    model3_statuses.append(statuses[2])\n",
    "\n",
    "df[\"model1_similarity\"] = model1_scores\n",
    "df[\"model1_status\"] = model1_statuses\n",
    "df[\"model2_similarity\"] = model2_scores\n",
    "df[\"model2_status\"] = model2_statuses\n",
    "df[\"model3_similarity\"] = model3_scores\n",
    "df[\"model3_status\"] = model3_statuses\n",
    "\n",
    "print(model1_scores, model1_statuses, model2_scores, model2_statuses, model3_scores, model3_statuses)\n",
    "\n",
    "df.to_csv(\"LLMs Answers_with_similarity.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
